{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28668c8e",
   "metadata": {},
   "source": [
    "## Reddit API Scraper via Reddit Praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb205ea2",
   "metadata": {},
   "source": [
    "In this notebook, you will see how this app scraps from Reddit using the Reddit Praw API. Additionally, all scrapped data will be placed into a pandas dataframe for further use in the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4a4ff",
   "metadata": {},
   "source": [
    "### Setup for Praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd30219",
   "metadata": {},
   "source": [
    "Here, we will import all necessary libraries for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efab9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d55b03",
   "metadata": {},
   "source": [
    "\n",
    "Below is the set up for an API agent that will used to surf Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4fea650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected as user: None\n"
     ]
    }
   ],
   "source": [
    "# Initialize PRAW with your Reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"Wh4ZlWWXXIgDh948wfr7XA\",\n",
    "    client_secret=\"KvRaTg25M0xF07AJuFcJWDcHlcwpng\",\n",
    "    user_agent=\"RUControversial/1.0 by /u/Such_Touch2846\",\n",
    ")\n",
    "\n",
    "# User: None indicates that the script is logged in as the authenticated user without posting permissions\n",
    "print(f\"Connected as user: {reddit.user.me()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1f128",
   "metadata": {},
   "source": [
    "### Subreddit Scrapping: r/AITA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c975113",
   "metadata": {},
   "source": [
    "After successfully connecting to the API, we will now select a subreddit, r/AITA for our project, and begin to pull posts. We will grab 25 posts from each category\n",
    "\n",
    "Categories\n",
    "- Hot\n",
    "- Recent\n",
    "- Top\n",
    "\n",
    "Please excuse the subreddit's title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690bc8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Flairs Found:\n",
      "Asshole\n",
      "Charitable META\n",
      "Everyone Sucks\n",
      "META\n",
      "No A-holes here\n",
      "Not enough info\n",
      "Not the A-hole\n",
      "Not the A-hole POO Mode\n",
      "UPDATE\n",
      "Update\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit(\"AmItheAsshole\")  # Example subreddit\n",
    "\n",
    "# Define valid verdicts\n",
    "# Define valid verdicts with their meanings\n",
    "valid_verdicts = [\n",
    "    \"Asshole\",\n",
    "    \"Not the A-hole\",\n",
    "    \"Everyone Sucks\",\n",
    "    \"No A-holes here\"\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize a set to store unique flairs\n",
    "unique_flairs = set()\n",
    "\n",
    "\n",
    "\n",
    "MAX_POSTS_PER_VERDICT = 25\n",
    "posts_by_verdict = defaultdict(list)\n",
    "fetched_count = defaultdict(int)\n",
    "collected_post_ids = set()\n",
    "\n",
    "# Function to collect posts\n",
    "def collect_posts(submissions):\n",
    "    for submission in submissions:\n",
    "        # Stop if all verdicts have reached the max\n",
    "        if all(fetched_count[verdict] >= MAX_POSTS_PER_VERDICT for verdict in valid_verdicts):\n",
    "            break\n",
    "        if submission.id in collected_post_ids:\n",
    "            continue\n",
    "        flair = submission.link_flair_text\n",
    "        if flair and flair in valid_verdicts and fetched_count[flair] < MAX_POSTS_PER_VERDICT:\n",
    "            post_data = {\n",
    "                \"post_id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"subreddit_name\": submission.subreddit.display_name,\n",
    "                \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                \"score\": submission.score,\n",
    "                \"upvote_ratio\": submission.upvote_ratio,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"created_utc\": dt.datetime.fromtimestamp(submission.created_utc, dt.timezone.utc),\n",
    "                \"flair\": submission.link_flair_text,\n",
    "                \"url\": submission.url,\n",
    "                \"is_self\": submission.is_self,\n",
    "                \"label\": submission.link_flair_text\n",
    "            }\n",
    "            posts_by_verdict[flair].append(post_data)\n",
    "            fetched_count[flair] += 1\n",
    "            collected_post_ids.add(submission.id)\n",
    "            print(f\"Collected '{flair}': {fetched_count[flair]}\")\n",
    "\n",
    "# Collect posts until we have 25 for each flair\n",
    "try:\n",
    "    collect_posts(subreddit.new(limit=5000))\n",
    "except praw.exceptions.RedditAPIException as e:\n",
    "    print(f\"API error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"General error: {e}\")\n",
    "\n",
    "# Combine posts into a list\n",
    "all_posts = []\n",
    "for verdict_posts in posts_by_verdict.values():\n",
    "    all_posts.extend(verdict_posts)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPosts per flair:\", dict(fetched_count))\n",
    "print(\"Total number of posts:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae1542",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f6787",
   "metadata": {},
   "source": [
    "After pulling the reddit posts, we will now convert the data to a csv and export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee04523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aita_posts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babcd995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
